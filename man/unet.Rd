% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/unet.R
\name{unet}
\alias{unet}
\title{UNet model for spatial downscaling using deep learning}
\usage{
unet(
  coarse_data,
  fine_data,
  time_points = NULL,
  val_coarse_data = NULL,
  val_fine_data = NULL,
  val_time_points = NULL,
  cyclical_period = NULL,
  cycle_onehot = FALSE,
  cos_sin_transform = FALSE,
  temporal_basis = c(9, 17, 37),
  temporal_layers = c(32, 64, 128),
  initial_filters = c(16),
  initial_kernel_sizes = list(c(3, 3)),
  filters = c(32, 64, 128),
  kernel_sizes = list(c(3, 3), c(3, 3), c(3, 3)),
  use_batch_norm = FALSE,
  dropout_rate = 0.2,
  activation = "relu",
  final_activation = "linear",
  optimizer = "adam",
  learning_rate = 0.001,
  loss = "mse",
  metrics = c(),
  batch_size = 32,
  epochs = 100,
  validation_split = 0.2,
  normalize = TRUE,
  callbacks = NULL,
  seed = NULL,
  verbose = 1
)
}
\arguments{
\item{coarse_data}{3D or 4D array. The coarse resolution input data in format
\verb{[x, y, variables, time]}, where the variables dimension is optional.}

\item{fine_data}{3D or 4D array. The fine resolution target data in format
\verb{[x, y, variables, time]}, where the variables dimension is optional.}

\item{time_points}{Numeric vector. Optional time points corresponding to each time step in the data.}

\item{val_coarse_data}{An optional 3D or 4D array of coarse resolution input data in format
\verb{[x, y, variables, time]}, where the variables dimension is optional.}

\item{val_fine_data}{An optional 3D or 4D array of fine resolution target data in format
\verb{[x, y, variables, time]}, where the variables dimension is optional.}

\item{val_time_points}{An optional numeric vector of length n representing the time points of the validation samples.}

\item{cyclical_period}{Numeric. Optional period for cyclical time encoding (e.g., 365 for yearly seasonality).}

\item{cycle_onehot}{Boolean. If TRUE, a onehot encoded vector of temporal cycles is added as input to temporal module.}

\item{cos_sin_transform}{Logical. Whether to use cosine-sine transformation for time features. Default: FALSE.}

\item{temporal_basis}{A numeric vector specifying the temporal basis functions to use for time encoding (default is c(9, 17, 37)).}

\item{temporal_layers}{A numeric vector specifying the number of units in each dense layer for time encoding (default is c(32, 64, 128)).}

\item{initial_filters}{Integer vector. Number of filters in the initial convolutional layers. Default: c(16).}

\item{initial_kernel_sizes}{List of integer vectors. Kernel sizes for the initial convolutional layers. Default: list(c(3, 3)).}

\item{filters}{Integer vector. Number of filters in each convolutional layer. Default: c(32, 64, 128).}

\item{kernel_sizes}{List of integer vectors. Kernel sizes for each convolutional layer. Default: list(c(3, 3), c(3, 3), c(3, 3)).}

\item{use_batch_norm}{Logical. Whether to use batch normalization after convolutional layers. Default: FALSE.}

\item{dropout_rate}{Numeric. Dropout rate for regularization. Default: 0.2.}

\item{activation}{Character. Activation function for hidden layers. Default: "relu".}

\item{final_activation}{Character. Activation function for output layer. Default: "linear".}

\item{optimizer}{Character or optimizer object. Optimizer for training. Default: "adam".}

\item{learning_rate}{Numeric. Learning rate for optimizer. Default: 0.001.}

\item{loss}{Character or loss function. Loss function for training. Default: "mse".}

\item{metrics}{Optional character vector. Metrics to track during training.}

\item{batch_size}{Integer. Batch size for training. Default: 32.}

\item{epochs}{Integer. Number of training epochs. Default: 100.}

\item{validation_split}{Numeric. Fraction of data to use for validation. Default: 0.2.}

\item{normalize}{Logical. Whether to normalize data before training. Default: TRUE.}

\item{callbacks}{List. Keras callbacks for training. Default: NULL.}

\item{seed}{Integer. Random seed for reproducibility. Default: NULL.}

\item{verbose}{Integer. Verbosity mode (0, 1, or 2). Default: 1.}
}
\value{
List containing the trained model and associated components:
\item{model}{Trained Keras model}
\item{input_mask}{Mask for input data based on the missing values}
\item{target_mask}{Mask for target data based on the missing values}
\item{min_time_point}{Minimum time point in the training data}
\item{max_time_point}{Maximum time point in the training data}
\item{cyclical_period}{Cyclical period for time encoding}
\item{max_season}{Maximum season for time encoding}
\item{axis_names}{Names of the axes in the input data}
\item{history}{Training history}
}
\description{
Implements a time-aware UNet convolutional neural network for spatial downscaling of grid data.
Time-aware UNet features an encoder-decoder architecture with skip connections and a temporal module.
The function allows an option for adding a temporal module for spatio-temporal applications.
}
\details{
The UNet architecture \insertCite{ronneberger2015u}{SpatialDownscaling} is widely used in image processing
tasks and has recently been adopted for spatial downscaling applications
\insertCite{sha2020deep}{SpatialDownscaling}. The method implemented here consists of:
\enumerate{
\item \strong{Initial Upscaling} – Coarse-resolution inputs are first upsampled using
bilinear interpolation to match the spatial dimensions of the fine-resolution target.
\item \strong{Initial Feature Extraction} – Multiple convolutional layers extract
low-level features before entering the encoder path.
\item \strong{Encoder Path} – A sequence of convolutional blocks with max-pooling
reduces spatial dimensions while increasing feature depth.
\item \strong{Decoder Path} – Spatial resolution is recovered via bilinear upsampling
and convolutional layers. Skip connections from the encoder help preserve
fine-scale information.
\item \strong{Skip Connections} – These link encoder and decoder layers at matching
resolutions, improving gradient flow and retaining fine spatial structure.
\item \strong{Temporal Module (optional)} – Time information can be incorporated
through cosine–sine encoding, one-hot seasonal encoding, or radial-basis
temporal features. These are passed through dense layers and reshaped to
merge with the UNet bottleneck.
}

The function supports missing data via masking, optional normalization,
validation data, and configurable UNet depth and width.
}
\examples{
\dontrun{
library(keras3)

# Create tiny dummy data:
# Coarse grid: 8x8 → Fine grid: 16x16
nx_c <- 8; ny_c <- 8
nx_f <- 16; ny_f <- 16
T <- 5  # number of time steps

# Coarse data:
coarse_data <- array(runif(nx_c * ny_c * 1 * T),
                     dim = c(nx_c, ny_c, 1, T))

# Fine data:
fine_data <- array(runif(nx_f * ny_f * 1 * T),
                   dim = c(nx_f, ny_f, 1, T))

# Optional time points
time_points <- 1:T

# Fit a tiny UNet (very small filters to keep the example fast)
model_obj <- unet(
  coarse_data,
  fine_data,
  time_points = time_points,
  filters = c(8, 16),
  initial_filters = c(4),
  epochs = 2,
  batch_size = 2,
  verbose = 0
)
}

}
\references{
\insertAllCited{}
}
